\documentclass{report}
\usepackage[T1]{fontenc}
\usepackage{jaytex}
\renewcommand\chaptername{Lecture}

\begin{document}

\title{Lecture notes for \textit{EE 123: Digital Signal Processing} lectured by Kannan Ramchandran, Spring 2021}
\author{Jay Monga}
\maketitle

\tableofcontents

\chapter{Introduction}
Digital signal processing is a broad field that has existed for quite some time but remains hugely important to date.
Prof. Lustig likes to call it the "swiss-army knife" for scientists and engineers, and also says you can do a lot of badass things with DSP like steal cars. \\
This course will extend off of EE 120 to cover many aspects of digital signal processing, in both theory and implementation.
There will be a significant lab and project component of this course, although we won't be doing any HAM radio things as usual.

\section{Review of Common Signals}
We will first talk about common signals you should be familiar with.
But before then, let's make sure we remember the difference between continuous time, discrete time, and digital signals.
\theoremstyle{definition}
\begin{definition}
    \textbf{Continuous-time signals} are functions of a continuous independent variable. These will be written as $x(t)$.
\end{definition}
\begin{definition}
    \textbf{Discrete-time signals} are sequences of numbers. These will be written as $x[n]$.
\end{definition}
\begin{definition}
    \textbf{Digital signals} are sequences of numbers with quantized amplitudes.
\end{definition}
Here are some common signals you should be familiar with
\begin{itemize}
    \item unit impulse
    \item unit step
    \item sinusoid
    \begin{itemize}
        \item concept of period for discrete and continuous time
    \end{itemize}
\end{itemize}

\section{Systems Review}
In EE 120, we dealt mainly with LTI, linear time-invariant, systems. Let's define these terms.

\theoremstyle{definition}
\begin{definition}
    A \textbf{linear} system $T$ must satisfy $T\{x + y\} = T\{x\} + T\{y\}$ for any inputs $x, y$.
\end{definition}

\begin{definition}
    A \textbf{time-invariant} system $T$ must satisfy $T\{x(n - \tau)\} = T\{x\}(n - \tau)$ for any input $x$ and delay $\tau$.
\end{definition}

There are other important classifations of these systems as well.

\begin{definition}
    \textbf{Causal} sytems produce an output that is only dependent on current and prior values of the input.
\end{definition}
\begin{definition}
    \textbf{Memory-less} sytems produce an output that is only dependent on the current value of the input.
\end{definition}
\begin{definition}
    A signal $x$ is \textbf{bounded} if and only if we can define some finite value $c$ such that $\forall t, \abs{x(t)} \leq c$
\end{definition}
\begin{definition}
    A sytem is said to be \textbf{BIBO stable} if there does not exist any bounde input to the system that will produce an unbounded output.
\end{definition}

Let's look at some examples.

\begin{example}
    Consider the system $T\{x\}(n) = x(n - \tau)$. This system is LTI, causal, not memory-less, and BIBO stable.
\end{example}

\begin{example}
    Consider the system $T\{x\}(n) = \s{i}{-\infty}{n}{x(i)}$. This system is LTI, causal, not memory-less, and BIBO stable.
\end{example}

\begin{example}
    Consider the system $T\{x\}(n) = x[m \cdot t]$ for some scalar $m$. This system is linear, not time-invariant, not causal, not memory-less, and BIBO stable.
\end{example}

\chapter{120 Review Cont.}

\section{LTI Recap}
\subsection{Linearity and Time Invariance}
As discussed previously, we can prove linearity of a system by proving that it satisfies properties of superposition and scaling.

\theoremstyle{definition}
\begin{example}
    Consider a 3 point moving average filter, $y[n] = \frac{1}{3} (x[n - 1] + x[n] + x[n + 1])$. This represents a linear system.
\end{example}
\begin{example}
    Consider a 3 point median filter, $y[n] = \mathrm{Median}(x[n - 1], x[n], x[n + 1])$. This represents a non-linear system. Note that this non-linear filter still is used a lot in practice, since it's good at removing "shot noise".
\end{example}
Recall that we also need time-invariance to hold for a system to be LTI.
\subsection{Impulse Response}
\begin{definition}
    The \textbf{impulse response} of a system is the output of the system when the input is the unit impulse $\delta[n]$. We often label this output as $h[n]$.
\end{definition}
LTI systems are completely characterized by their impulse response.
\subsubsection{Output in terms of Impulse Reponse}
% Need to write convolution formula
% Need to write out proof
\begin{proof}
    Start
\end{proof}
\subsubsection{Stability in terms of Impulse Response}
\begin{theorem}
    BIBO stability in LTI Systems $\iff$ Impulse response is absolutely summable $\s{n}{-\infty}{\infty}{\abs{h[n]}}$
\end{theorem}

\subsubsection{Causality and Memoryless Properties}
We can also show that these properties of LTI systems can be determiend through analysis of the impulse response.

\subsection{Eigenfunctions of LTI systems}
Recall the concept of eigenvectors
% Show scaling by linear transformation
Similarly, we may define eigenfunctions of LTI systems to. In contrast to linear algebra, we may identify all eigenfucntions of an LTI system quite easily since they all take on the form $e^{j \omega n}$
\begin{definition}
    The \textbf{frequency response} of an LTI system is the function $H(e^{j \omega}) = \s{n}{-\infty}{\infty}{h[n] e^{-j \omega n}}$. This is also sometimes refered to as the \textbf{transfer function}.
\end{definition}
\begin{theorem}
    The output of an LTI system to input $e^{j \omega n}$ us $H(e^{j\omega}) e^{j\omega}$
\end{theorem}
\begin{proof}
    %Need to do this
\end{proof}
\section{Discrete-Time Fourier Transform (DTFT)}
Discussion of the frequency response now brings us to the DTFT. 
\begin{definition}
    The \textbf{discrete-time fourier transform} of a signal is $\mathrm{DTFT}\{x[n]\} = X(e^{j\omega}) = \s{n}{-\infty}{\infty}{x[n] e^{-j\omega n}}$. We can also define the \textbf{inverse discrete-time fourier transform} as $x[n] = \frac{1}{2 \pi} \I{\omega}{-\pi}{\pi}{X(e^{j\omega n})}$
\end{definition}
We can see how the DTFT definition is motivated by the definition of the frequency response from earlier.

The DTFT has a nice dual to the classical fourier series.
\begin{definition}
    %Put down fourier series formula
    The \textbf{fourier series} of a continuous signal is $f_{T}(t) = \s{-\infty}{\infty}{k}{}$
\end{definition}
% explain that the fourier series is just DTFT formula but swap frequency and time and period is 2 pi
 
\theoremstyle{definition}
\begin{example}
    Define a signal
    \begin{equation*}
        w[n] = \begin{cases}
            1 & 0 \leq n \leq N - 1 \\
            0 & \text{else}
        \end{cases}
    \end{equation*}
    %include picture
    This is a finite impulse train.
    We will compute the DTFT of this signal
    \begin{align*}
        W(e^{j\omega}) &= \s{k}{0}{N - 1}{e^{-j \omega k}} \\
        &= \s{k}{0}{N - 1}{\alpha^k} \\
        &= \frac{1 - \alpha^N}{1 - \alpha} \\
        &= \frac{1 - e^{-j \omega N}}{1 - e^{- j \omega}} \\
        &= \frac{e^{-j \omega \frac{N}{2}} [e^{-j \omega \frac{N}{2}} - e^{j \omega \frac{N}{2}}]}{} \\
        % more algebra...
        \Aboxed{W(e^{j \omega}) &= \abs{\frac{}{}}}
        %Fix this - you get a periodic sinc
        % because things in DTFT need to be periodic
        % add a picture of the periodic sinc
    \end{align*}
\end{example}
\subsubsection{Properties}
\begin{itemize}
    \item Convolution in time is multiplication in frequency
    \begin{equation*}
        y[n] = (x * h)[n] \leftrightarrow Y(e^{j \omega n}) = H(e^{j \omega}) X(e^{j \omega})
    \end{equation*}
    \item Conjugate symmetry: If $x[n]$ is real...
    \begin{equation*}
        X^*(e^j \omega) = X(e^{- j \omega n})
    \end{equation*}
    We can actually exploit this redundancy in the requency specturm to save time and money
    \item Parseval's Theorem
    First, we state generalized Parseval's theorem
    \begin{equation*}
        \s{n}{-\infty}{\infty}{x[n] * y[n]} = \frac{1}{2 \pi} \I{\omega}{-\infty}{\infty}{X(e^{j \omega}) Y^*({e ^{- j \omega}})}
    \end{equation*}
    and then here is a form that shows how energy is preserved
    % need to add this
    \item Shift property
\end{itemize}

\chapter{Transforms of Discrete Signals}
\section{DTFT Continued}
Refer above to more in depth of these DTFT properties
\begin{itemize}
    \item Analysis and synthesis equations
    \item Relationship between DTFT and FFT
    \item Window function and periodic sinc
    \item Convolution in time is multiplication in frequency
    \item Conjugate symmetry for real signals
    \item Parseval's theorem tells us that energy is conserved (with a factor of ($2 \pi$)
    \item Shift property
\end{itemize}

\begin{example}
    Consider an ideal low pass filter box function from $-\omega_c$ to $\omega_c$. 
    % put in math
    We get
    \begin{equation*}
        h[n] = \frac{\omega_c}{\pi} \frac{sin(\omega_c n)}{\omega_c n}
    \end{equation*}
    Cool! Recall that we see the $\mathrm{sinc}$ function here.
    \begin{equation*}
        \mathrm{sinc}(x) = 
        \begin{cases}
            \frac{sin(x)}{x} & x \neq 0 \\
            1 & x = 0 \\
        \end{cases}
    \end{equation*}
    Actually not cool. Looking at the impulse response
    % put plot of impulse response
    We see there are two problems that make this ideal not realizable
    \begin{itemize}
        \item $h[n]$ is not causal
        \item $h[n]$ is infinite
    \end{itemize}
    We can fix this by adding a delay to our impulse response and applying a windowing function. What would the effects on the frequency response? We can use the shift property to see what the delay does, but for the multiplication by the windowing function we derive a relationship between multiplication in time and convolution in frequency
    %  add derivation
    % continue discussion of effects, side lobes, gibs phehonoem, add picture
\end{example}

\section{Convergence}

\begin{definition}
    % If $\exists$ a continuous function of \omega, called $X(e^{j \omega})$ s.t. $\forall \epsilon > 0, \forall \omega, \exists n_0$ s.t. $\abs {\s{n}{-n_0}{n_0}{x[n] e^{-j \omega n} - X(e^j \omega n)}}$ % fix error here and complete definition
\end{definition}

This form of convergence might be too strict and will tell us that a lot of things don't converge. Thus, we must introuce MSE convergence.

\begin{definition}
    % Explain convergence in mean squared error
\end{definition}

\begin{theorem}
    $\s{n}{- \infty}{\infty}{\abs{x[n]}} < \infty \implies X(e^{j \omega})$ converges uniformly
\end{theorem}

\begin{theorem}
    $\s{n}{- \infty}{\infty}{\abs{x[n]}^2} < \infty \implies X(e^{j \omega})$ converges in MSE
\end{theorem}

\begin{example}
    % Go through sinc function example
    % It won't converge uniformly
    % But it will converge MSE
\end{example}

\section{Z-Transform}
Motvation:
\begin{itemize}
    \item DTFT doesn't always converge, so we need another tool in our toolbox
    \item Solve difference equations
    \item Can use it for the same analysis we were doing with the DTFT
\end{itemize}

Similarly to how $e^{j\omega n}$ behaves as an eigenfunction to LTI systems, we can see that the general exponential $z^n$ behaves the same way.
% show eigenfunction property of z^n$
The Z-transform of the impulse response gives us the transfer function of the system.
% add picture, show relation to DTFT

\subsection{Region of Convergence (ROC)}
% Do the definition

\begin{example}
    Consider a right sided sequence $x[n] = a^n u[n]$. Consider its Z-transform
    \begin{equation*}
        X(z) = \s{n}{0}{\infty}a^n z^{-n} = \s{n}{0}{\infty}{(a z^{-1})^n}
    \end{equation*}
    Remembering what we know about geometric series, we see this converges to $X(z) = \frac{1}{1 - a z^{-1}}$ if $\abs{\frac{a}{z}} < 1$, or $\abs{a} < \abs{z}$.
    % draw picture
\end{example}

\begin{example}
    Consider the sum of two right sided sequences $x[n] = (\frac{1}{2})^n + (-\frac{1}{3})^n$. The DTFT of this signal should be
    \begin{equation*}
        X(z) = \frac{1}{1 - \frac{1}{2} z ^{-1}} + \frac{1}{1 + \frac{1}{3} z ^{-1}}
    \end{equation*}
    % Explain ROC as intersection of both ROC
\end{example}

\begin{example}
    Consider left sided sequence $x[n] = -a^n u[-n - 1]$
    % need to draw out this signal
    % Do derivation for X(Z), find out its the same as right sided
    % But the region of convergence is inverted
    % Point is, inverse Z transform is can be ambiguous without ROC specified
\end{example}

\begin{example}
    Consider a two-sided sequence $x[n] = -(\frac{1}{2})^n u[n] - (- \frac{1}{3})^n u[-n - 1]$
    % show that ROC must be a ring
\end{example}

\begin{example}
    Consider a two-sided sequence $x[n] = -(\frac{1}{2})^n u[n] - (- \frac{1}{3})^n u[-n - 1]$
    % show that ROC must be a ring
\end{example}

% I missed two examples >:(

\begin{example}
    Consider the finite-length $M$ sequence $x[n] = a^n u[n] u[-n + M - 1]$
    % show that, of course, this converges (excpet for \abs{z} = 0 I guess)
\end{example}

General properties to remember:
\begin{itemize}
    \item Generally, ROC will be a ring or disk in $Z$-plane centered at the origin
    \item DTFT converges $\iff$ ROC includes unit circle
    % I missed one
    \item For right-sided sequences, ROC extends from outermost pole to infinity. See this in examples 1, 2.
    % missed 2 more
\end{itemize}

Properties of DTFT seeem to also hold for Z transform
\begin{itemize}
    \item $x[n - n_d] \leftrightarrow z^{-n_d} X(z)$
    \item $z_0^n x[n] \leftrightarrow X(\frac{z}{z_0})$
    \item $nx[n] \leftrightarrow -z \diff{X(z)}{dz}$
    % Missed other properties, put in derivations too
\end{itemize}

Rigorous inversion of $z$-transform relies contour integration within the ROC. We can avoid it by
\begin{itemize}
    \item Inspection, utilizing known transforms
    \item Properties of the z-transform
    \item Power series expansion
    \item Parital fraction decomposition
    \item Residue theorem (partial fraction decomposition)
\end{itemize}
%something about inverse of rational polynomials

\chapter{Z-Transform and DFT}
Readings: Chapter 3 and 8

\section{Z-Transform Recap}

\subsection{Properties}
Recall the bi-lateral z-transform
\begin{equation*}
    X(z) = \s{n}{\infty}{\infty}{x[n]z^{-n}}, z \in \mathrm{R.O.C}(x)
\end{equation*}

The ROC will take the shape of a ring

\begin{align*}
    X(z)
    &= \s{n}{-\infty}{-1}{x[n]z^[-n]} & \; \text{region of convergence} \abs{z} < \abs{a} \\
    &= + \s{n}{0}{\infty}{x[n]z^[-n]} & \text{region of convergence} \; \abs{z} > \abs{a} \\
\end{align*}

Other notes

% fill these in
\begin{itemize}
    \item ROC and inverse $Z$-transform
    \item Linearity
    \item Pole-zero cancelation
    \item Convolution
\end{itemize}
Read the book for more detail

\begin{example}[Transfor function $H(z)$ of a DT system]
    % Need to add picture
    % show how to get transfer function
    % solve for transfer function
    % solve for h[n]
\end{example}

\subsection{Inversion of Z-Transform}
There is an inverse formula, but you need complex analysis to understand it. Instead, we proceed by

% need to fill out these things
\begin{itemize}
    \item Inspection
    % Right-sided exponential
    \item Properties of $Z$-transform
    \begin{itemize}
        \item Delays
        % Delayed exponential
        % Sum of delays (power series)
        % Exercise: Find inverse z of log(1 + az^-1)
    \end{itemize}
    \item Partial Fraction Decomposition (PFE)
    % put down the example he used
    % lol you need to relearn this
\end{itemize}

\begin{theorem}
    If $H(z)$ is a rational transfer function,
    \begin{equation*}
        \text{BIBO stability } \iff \text{ ROC includes unit circle}
    \end{equation*}
    \begin{proof}
        \begin{description}
            \item[If]
        % Trivial, fill it out later
            \item[Only If] 
        BIBO stable means that $h[n]$ is absolutely summable, so the $DTFT$ exists. Thus, $Z$-transform will include unit circle
        \end{description}
    \end{proof}
\end{theorem}

% need to do corollary connection between stability and poles

\section{Discrete Fourier Transform (DFT)}
\subsection{Motivaton}

\begin{itemize}
    \item DTFT is nice, we have good continuous figure of the frequency spectrum

    \item But not good to do on a computer
    \begin{itemize}
        \item Infinite sum!
        \item Continuous signal!
    \end{itemize}
    % missed more things here...

    \item Efficient to do on a computer!
    \begin{itemize}
        \item Direct evaluation is $O(N^2)$
        \item FFT is $O(N \log N)$
        \item FFTW library
        \item Use it to do convolution!
        \begin{itemize}
            \item Direct convolution is $O(N^2)$
            \item FFT-based convolution is $O(N \log N)$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Explanation}
Assume we have the DFT definition

\begin{equation*}
    X[k] = \s{n}{0}{N-1}{x[n]e^{-j \frac{2 \pi}{N} n k}}
\end{equation*}
This looks similar to the DTFT! In fact

% probably want to give proof for this
\begin{equation*}
    X[k] = \eval{X(e^{j \omega})}{\omega = \frac{2 \pi}{N} k}{}
\end{equation*}
We can see prove that the DFT is inherrently periodic
\begin{align*}
    X[k + rN] 
    &= \s{n}{0}{N-1}{e^{-j \frac{2 \pi}{N} (k + rN)}} \\
    &= X[k] \cdot e^{-j \frac{2 \pi}{N} rN} \\
    &= X[k]
\end{align*}

\subsection{Inverse}
We will start with a formula for the DFT inverse
\begin{equation*}
    x[n] = \frac{1}{N} \s{k}{0}{N - 1}{X[k] e^{j \frac{2 \pi}{N} nk}}
\end{equation*}
and prove that it works
\begin{proof}
    \begin{align*}
        \frac{1}{N} \s{k}{0}{N - 1}{X[k] e^{j \frac{2 \pi}{N} nk}}
        &= \frac{1}{N} \s{k}{0}{N - 1}{\s{n}{0}{N - 1}{x[n] e^{-j \frac{2 \pi}{N}n k}} e^{j \frac{2 \pi}{N} nk}} \\
        &= \frac{1}{N} \s{n}{0}{N - 1}{x[n] \s{k}{0}{N - 1}{e^{-j \frac{2 \pi}{N}n k}} e^{j \frac{2 \pi}{N} nk}} \\
    \end{align*}
\end{proof}
%This needs to be cleaned up
To simplify things, we will define
\begin{equation*}
    W_n = e^{-j \frac{2\pi}{N}}
\end{equation*}
% Rewrite formula with W_N notation
% Explan synthesis and analysis
% Explain invertible transformation between signal and DFT
\end{document}